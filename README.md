# 使用 nextjs 本地化部署 AI 大模型 gemma

**本案例使用到：ollama + nextjs + langchain.js + milvus**

**​ollama：​**本地运行模型服务

**​nextjs：​**[前端框架](https://so.csdn.net/so/search?q=%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6&spm=1001.2101.3001.7020)项目

**​langchain.js：​**调用模型服务并对话

**​milvus：​**向量[数据库](https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%BA%93&spm=1001.2101.3001.7020)

详细介绍请看博客：[https://blog.csdn.net/qq_34330989/article/details/136562999](https://blog.csdn.net/qq_34330989/article/details/136562999)

---

###### 安装依赖：

```
yarn
```

###### 本地运行：

```
yarn run dev
```

###### 打开地址查看：

```
http://localhost:3000
```

有任何问题，可以提交到 issues

联系我 V：Gold_hj
